{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e410a2ab-8fa6-4125-8f7c-a7472ecca407",
   "metadata": {},
   "source": [
    "# Lab 05 Dropout and batch normalization\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"img/craiyon-normal.jpg\" alt=\"Spam\" width=\"300\"/> \n",
    "    \n",
    "    Craiyon.com \"Normalization\" \n",
    "    \n",
    "</center>    \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b1226",
   "metadata": {
    "papermill": {
     "duration": 0.008322,
     "end_time": "2022-05-05T17:51:18.977696",
     "exception": false,
     "start_time": "2022-05-05T17:51:18.969374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1 Objectives\n",
    "\n",
    "There's more to the world of deep learning than just dense layers. There are dozens of kinds of layers you might add to a model. (Try browsing through the [Keras docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/) for a sample!) Some are like dense layers and define connections between neurons, and others can do preprocessing or transformations of other sorts.\n",
    "\n",
    "In this lesson, we'll learn about a two kinds of special layers, not containing any neurons themselves, but that add some functionality that can sometimes benefit a model in various ways. Both are commonly used in modern architectures.\n",
    "\n",
    "## 2  Dropout\n",
    "\n",
    "The first of these is the \"dropout layer\", which can help correct overfitting.\n",
    "\n",
    "In the last lesson we talked about how overfitting is caused by the network learning spurious patterns in the training data. To recognize these spurious patterns a network will often rely on very a specific combinations of weight, a kind of \"conspiracy\" of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.\n",
    "\n",
    "This is the idea behind **dropout**. To break up these conspiracies, we randomly *drop out* some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\n",
    "\n",
    "<center>\n",
    "    <figure style=\"padding: 1em;\">\n",
    "<img src=\"img\\5-1.gif\" width=\"600\" alt=\"An animation of a network cycling through various random dropout configurations.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\">Here, 50% dropout has been added between the two hidden layers.</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "You could also think about dropout as creating a kind of *ensemble* of networks. The predictions will no longer be made by one big network, but instead by a committee of smaller networks. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you're familiar with random forests as an ensemble of decision trees, it's the same idea.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0acf0-de02-425f-9492-a7068fbef1a9",
   "metadata": {},
   "source": [
    "## 3 Adding Dropout\n",
    "\n",
    "In Keras, the dropout rate argument `rate` defines what percentage of the input units to shut off. Put the `Dropout` layer just before the layer you want the dropout applied to:\n",
    "\n",
    "```\n",
    "keras.Sequential([\n",
    "    # ...\n",
    "    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n",
    "    layers.Dense(16),\n",
    "    # ...\n",
    "])\n",
    "```\n",
    "\n",
    "## 4 Batch Normalization\n",
    "\n",
    "The next special layer we'll look at performs \"batch normalization\" (or \"batchnorm\"), which can help correct training that is slow or unstable.\n",
    "\n",
    "With neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) or [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\n",
    "\n",
    "Now, if it's good to normalize the data before it goes into the network, maybe also normalizing inside the network would be better! In fact, we have a special kind of layer that can do this, the **batch normalization layer**. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\n",
    "\n",
    "Most often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance). Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get \"stuck\". Consider adding batch normalization to your models, especially if you're having trouble during training.\n",
    "\n",
    "## 5 Adding Batch Normalization \n",
    "\n",
    "It seems that batch normalization can be used at almost any point in a network. You can put it after a layer...\n",
    "\n",
    "```\n",
    "layers.Dense(16, activation='relu'),\n",
    "layers.BatchNormalization(),\n",
    "```\n",
    "\n",
    "... or between a layer and its activation function:\n",
    "\n",
    "```\n",
    "layers.Dense(16),\n",
    "layers.BatchNormalization(),\n",
    "layers.Activation('relu'),\n",
    "```\n",
    "\n",
    "And if you add it as the first layer of your network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn's `StandardScaler`.\n",
    "\n",
    "## 6 Example - Using Dropout and Batch Normalization\n",
    "\n",
    "Let's continue developing the *Red Wine* model. Now we'll increase the capacity even more, but add dropout to control overfitting and batch normalization to speed up optimization. This time, we'll also leave off standardizing the data, to demonstrate how batch normalization can stabalize the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d14f52",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.061793,
     "end_time": "2022-05-05T17:51:19.046929",
     "exception": false,
     "start_time": "2022-05-05T17:51:18.985136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setup plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "red_wine = pd.read_csv('data/red-wine.csv')\n",
    "\n",
    "# Create training and validation splits\n",
    "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
    "df_valid = red_wine.drop(df_train.index)\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_train.drop('quality', axis=1)\n",
    "X_valid = df_valid.drop('quality', axis=1)\n",
    "y_train = df_train['quality']\n",
    "y_valid = df_valid['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd404fa",
   "metadata": {
    "papermill": {
     "duration": 0.007086,
     "end_time": "2022-05-05T17:51:19.061794",
     "exception": false,
     "start_time": "2022-05-05T17:51:19.054708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "When adding dropout, you may need to increase the number of units in your `Dense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c396a1",
   "metadata": {
    "papermill": {
     "duration": 6.696835,
     "end_time": "2022-05-05T17:51:25.766053",
     "exception": false,
     "start_time": "2022-05-05T17:51:19.069218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d16491",
   "metadata": {
    "papermill": {
     "duration": 0.00773,
     "end_time": "2022-05-05T17:51:25.782261",
     "exception": false,
     "start_time": "2022-05-05T17:51:25.774531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "There's nothing to change this time in how we set up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc64a6e",
   "metadata": {
    "papermill": {
     "duration": 34.059644,
     "end_time": "2022-05-05T17:51:59.850357",
     "exception": false,
     "start_time": "2022-05-05T17:51:25.790713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# might need colab\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "\n",
    "# Show the learning curves\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a7fe2-853c-48df-9be1-d5ae3a2fe932",
   "metadata": {},
   "source": [
    "## 7 Exercises\n",
    "\n",
    "In this exercise, you'll add dropout to the *Spotify* data model and see how batch normalization can let you successfully train models on difficult datasets.\n",
    "\n",
    "Run the next cell to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ac96e1-857e-45c6-8462-f50cc18c39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('animation', html='html5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4d5d1-df8c-4c5b-b9ca-de8c7efea79a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "First load the *Spotify* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb954cf4-24c7-4150-a90b-d7397fc5d3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [18]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "spotify = pd.read_csv('data/spotify.csv')\n",
    "\n",
    "X = spotify.copy().dropna()\n",
    "y = X.pop('track_popularity')\n",
    "artists = X['track_artist']\n",
    "\n",
    "features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n",
    "                'speechiness', 'acousticness', 'instrumentalness',\n",
    "                'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "features_cat = ['playlist_genre']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "def group_split(X, y, group, train_size=0.75):\n",
    "    splitter = GroupShuffleSplit(train_size=train_size)\n",
    "    train, test = next(splitter.split(X, y, groups=group))\n",
    "    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "y_train = y_train / 100\n",
    "y_valid = y_valid / 100\n",
    "\n",
    "input_shape = [X_train.shape[1]]\n",
    "print(\"Input shape: {}\".format(input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae28b45-4311-4ef2-ac90-95b833416844",
   "metadata": {},
   "source": [
    "### 7.1 Add Dropout to Spotify Model\n",
    "\n",
    "Here is a model: Add two dropout layers, one after the `Dense` layer with 128 units, and one after the `Dense` layer with 64 units. Set the dropout rate on both to `0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366c014-bafb-4555-85c6-057fc187bf8c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Add two 30% dropout layers, one after 128 and one after 64\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f9997-ff7a-4d79-9c44-9de6942b8ed4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now run this next cell to train the model see the effect of adding dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67c33e-b063-4d6b-8576-616fd441be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=50,\n",
    "    verbose=0,\n",
    ")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c667a71-b159-49be-8966-60eaf01d531b",
   "metadata": {},
   "source": [
    "### 7.2 Evaluate Dropout\n",
    "\n",
    "Remember this model tended to overfit the data around epoch 5. Did adding dropout seem to help prevent overfitting this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6941f5-41a2-4115-8eb5-8f3348716acd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, we'll explore how batch normalization can fix problems in training.\n",
    "\n",
    "Load the *Concrete* dataset. We won't do any standardization this time. This will make the effect of batch normalization much more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25405cc7-2442-4cf6-af63-0c7bd2cf8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "concrete = pd.read_csv('data/concrete.csv')\n",
    "df = concrete.copy()\n",
    "\n",
    "df_train = df.sample(frac=0.7, random_state=0)\n",
    "df_valid = df.drop(df_train.index)\n",
    "\n",
    "X_train = df_train.drop('CompressiveStrength', axis=1)\n",
    "X_valid = df_valid.drop('CompressiveStrength', axis=1)\n",
    "y_train = df_train['CompressiveStrength']\n",
    "y_valid = df_valid['CompressiveStrength']\n",
    "\n",
    "input_shape = [X_train.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51559f-6473-4290-9e09-8a77f11c3521",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Run the following cell to train the network on the unstandardized *Concrete* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fba988-8106-4528-94c5-516308ad7adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Validation Loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),    \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='sgd', # SGD is more sensitive to differences of scale\n",
    "    loss='mae',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bbf0ab-aff4-425d-9d9d-8fb819e293b5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Did you end up with a blank graph? Trying to train this network on this dataset will usually fail. Even when it does converge (due to a lucky weight initialization), it tends to converge to a very large number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab15814-36c7-4f92-a567-b2b34df6f8be",
   "metadata": {},
   "source": [
    "### 7.3 Add Batch Normalization Layers\n",
    "\n",
    "Batch normalization can help correct problems like this.\n",
    "\n",
    "Add four `BatchNormalization` layers, one before each of the dense layers. (Remember to move the `input_shape` argument to the new first layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415b6a6-c155-46ba-9dd7-c11be2ddf77b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Add a BatchNormalization layer before each Dense layer\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79c4bb-d3d3-411f-8173-e89d08722244",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Run the next cell to see if batch normalization will let us train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53b57f-f431-446c-b7ea-e91ee08db764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mae',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "EPOCHS = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=64,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bfe89-780b-4e2a-9ae0-9dfdf8f1b784",
   "metadata": {},
   "source": [
    "### 7.4 Evaluate Batch Normalization\n",
    "\n",
    "Did adding batch normalization help?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 54.652201,
   "end_time": "2022-05-05T17:52:02.921622",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-05T17:51:08.269421",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
