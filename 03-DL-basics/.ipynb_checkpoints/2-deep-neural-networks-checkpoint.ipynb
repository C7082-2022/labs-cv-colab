{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7807df-244c-4bd2-9652-856cef979d98",
   "metadata": {},
   "source": [
    "# Lab 02 Deep neural networks\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"img/craiyon-nn.jpg\" alt=\"Spam\" width=\"300\"/> \n",
    "    \n",
    "    Craiyon.com \"Deep neural networks\" \n",
    "    \n",
    "</center>    \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61455f0c",
   "metadata": {
    "papermill": {
     "duration": 0.005178,
     "end_time": "2022-05-05T17:51:14.787015",
     "exception": false,
     "start_time": "2022-05-05T17:51:14.781837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1 Objectives\n",
    "\n",
    "This notebook looks at building neural networks capable of learning the complex kinds of relationships deep neural nets are famous for.\n",
    "\n",
    "We will cover:\n",
    "- *modularity*, building up a complex network from simpler functional units\n",
    "- computing a linear function \n",
    "- combine and modify these single units to model more complex relationships\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf391b-5aef-4b58-8119-6ab6aaa5cedd",
   "metadata": {},
   "source": [
    "## 2 Layers\n",
    "\n",
    "Neural networks typically organize their neurons into **layers**. When we collect together linear units having a common set of inputs we get a **dense** layer.\n",
    "\n",
    "<center>\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"img\\2-1.png\" width=\"300\" alt=\"A stack of three circles in an input layer connected to two circles in a dense layer.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\">A dense layer of two linear units receiving two inputs and a bias.\n",
    "</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "You could think of each layer in a neural network as performing some kind of relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways. In a well-trained neural network, each layer is a transformation getting us a little bit closer to a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eed5c8-596d-4ebf-a47b-0d3d02319790",
   "metadata": {},
   "source": [
    "### 2.1 Many Kinds of Layers\n",
    "\n",
    "A \"layer\" in Keras is a very general kind of thing. A layer can be, essentially, any kind of <em>data transformation</em>. Many layers, like the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\">convolutional</a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\">recurrent</a> layers, transform data through use of neurons and differ primarily in the pattern of connections they form. Others though are used for <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\">feature engineering</a> or just <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add\">simple arithmetic</a>. There's a whole world of layers to discover -- <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">check them out</a>!\n",
    "</blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16b659-0a7e-4632-b2d8-72692e788a47",
   "metadata": {},
   "source": [
    "## 3 The Activation Function\n",
    "\n",
    "It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something *nonlinear*. What we need are activation functions.\n",
    "\n",
    "<center>\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"img\\2-2.png\" width=\"400\" alt=\" \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\">Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we'll need to use activation functions. \n",
    "</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "An **activation function** is simply some function we apply to each of a layer's outputs (its *activations*). The most common is the *rectifier* function $max(0, x)$.\n",
    "\n",
    "<center>\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"img\\2-3.png\" width=\"400\" alt=\"A graph of the rectifier function. The line y=x when x>0 and y=0 when x<0, making a 'hinge' shape like '_/'.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\">ReLU (rectified linear unit) function\n",
    "</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The rectifier function has a graph that's a line with the negative part \"rectified\" to zero. Applying the function to the outputs of a neuron will put a *bend* in the data, moving us away from simple lines.\n",
    "\n",
    "When we attach the rectifier to a linear unit, we get a **rectified linear unit** or **ReLU**. (For this reason, it's common to call the rectifier function the \"ReLU function\".)  Applying a ReLU activation to a linear unit means the output becomes `max(0, w * x + b)`, which we might draw in a diagram like:\n",
    "\n",
    "<center>\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"img\\2-4.png\" width=\"250\" alt=\"Diagram of a single ReLU. Like a linear unit, but instead of a '+' symbol we now have a hinge '_/'. \">\n",
    "<figcaption style=\"textalign: center; font-style: italic\">ReLU (rectified linear unit) diagram\n",
    "</figcaption>\n",
    "</figure>\n",
    "</center>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b116abd",
   "metadata": {
    "papermill": {
     "duration": 0.003773,
     "end_time": "2022-05-05T17:51:14.795064",
     "exception": false,
     "start_time": "2022-05-05T17:51:14.791291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4 \"Stacking\" Dense Layers\n",
    "\n",
    "Now that we have some nonlinearity, let's see how we can stack layers to get complex data transformations.\n",
    "\n",
    "<center>\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"img/2-5.png\" width=\"450\" alt=\"An input layer, two hidden layers, and a final linear layer.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\">A stack of dense layers makes a \"fully-connected\" network.\n",
    "</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "The layers before the output layer are sometimes called **hidden** since we never see their outputs directly.\n",
    "\n",
    "Now, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fb3a2-1c8d-4bb8-81ac-d3021b98ce95",
   "metadata": {},
   "source": [
    "## 5 Building Sequential Models\n",
    "\n",
    "The `Sequential` model we've been using will connect together a list of layers in order from first to last: the first layer gets the input, the last layer produces the output. This creates the model in the figure above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fbbc9c",
   "metadata": {
    "papermill": {
     "duration": 6.657357,
     "end_time": "2022-05-05T17:51:21.456591",
     "exception": false,
     "start_time": "2022-05-05T17:51:14.799234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adf86b",
   "metadata": {
    "papermill": {
     "duration": 0.004598,
     "end_time": "2022-05-05T17:51:21.466319",
     "exception": false,
     "start_time": "2022-05-05T17:51:21.461721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Be sure to pass all the layers together in a list, like `[layer, layer, layer, ...]`, instead of as separate arguments. To add an activation function to a layer, just give its name in the `activation` argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2dbf0d-ab4d-44c9-a4de-daad1e2d0723",
   "metadata": {},
   "source": [
    "## 6 Exercises\n",
    "\n",
    "In the tutorial, we saw how to build deep neural networks by stacking layers inside a `Sequential` model. By adding an *activation function* after the hidden layers, we gave the network the ability to learn more complex (non-linear) relationships in the data.\n",
    "\n",
    "In these exercises, you'll build a neural network with several hidden layers and then explore some activation functions beyond ReLU. Run this next cell to set everything up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113bd1a5-3e7d-4346-b6fb-bd28f7bb3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd880c98-6802-4fb2-9518-9337c5f32edd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In the *Concrete* dataset, your task is to predict the compressive strength of concrete manufactured according to various recipes.\n",
    "\n",
    "Run the next code cell without changes to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0bbe6c4-26e0-402a-9c72-660190d29994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>BlastFurnaceSlag</th>\n",
       "      <th>FlyAsh</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>CoarseAggregate</th>\n",
       "      <th>FineAggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>CompressiveStrength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  BlastFurnaceSlag  FlyAsh  Water  Superplasticizer  CoarseAggregate  \\\n",
       "0   540.0               0.0     0.0  162.0               2.5           1040.0   \n",
       "1   540.0               0.0     0.0  162.0               2.5           1055.0   \n",
       "2   332.5             142.5     0.0  228.0               0.0            932.0   \n",
       "3   332.5             142.5     0.0  228.0               0.0            932.0   \n",
       "4   198.6             132.4     0.0  192.0               0.0            978.4   \n",
       "\n",
       "   FineAggregate  Age  CompressiveStrength  \n",
       "0          676.0   28                79.99  \n",
       "1          676.0   28                61.89  \n",
       "2          594.0  270                40.27  \n",
       "3          594.0  365                41.05  \n",
       "4          825.5  360                44.30  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "concrete = pd.read_csv('data/concrete.csv')\n",
    "concrete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebbbb4-6ec3-42da-9a91-61036ca0243f",
   "metadata": {},
   "source": [
    "### 6.1 Input Shape\n",
    "\n",
    "The target for this task is the column `'CompressiveStrength'`. The remaining columns are the features we'll use as inputs.\n",
    "\n",
    "What would be the input shape for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d311ff-6ff2-414a-bdf6-1f5fa2498145",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# input_shape = ____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e677d1d-63f2-452b-8ba7-794ea80fe2b6",
   "metadata": {},
   "source": [
    "### 6.2 Define a Model with Hidden Layers #\n",
    "\n",
    "Now create a model with three hidden layers, each having 512 units and the ReLU activation.  Be sure to include an output layer of one unit and no activation, and also `input_shape` as an argument to the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49938721-85eb-4882-9c9e-46a98ab4af4b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# model = ____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca47a7e-33f0-43a1-86d8-1836418a5d61",
   "metadata": {},
   "source": [
    "### 6.3 Activation Layers #\n",
    "\n",
    "Let's explore activations functions.\n",
    "\n",
    "The usual way of attaching an activation function to a `Dense` layer is to include it as part of the definition with the `activation` argument. Sometimes though you'll want to put some other layer between the `Dense` layer and its activation function. (We'll see an example of this in Lesson 5 with *batch normalization*.) In this case, we can define the activation in its own `Activation` layer, like so:\n",
    "\n",
    "```\n",
    "layers.Dense(units=8),\n",
    "layers.Activation('relu')\n",
    "```\n",
    "\n",
    "This is completely equivalent to the ordinary way: `layers.Dense(units=8, activation='relu')`.\n",
    "\n",
    "Rewrite the following model so that each activation is in its own `Activation` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539d6568-2664-43bc-bf69-d0625c8f50ff",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36424\\2791945415.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### YOUR CODE HERE: rewrite this to use activation layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model = keras.Sequential([\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE: rewrite this to use activation layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=[8]),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b636949-8401-4346-8db8-32f982692c59",
   "metadata": {},
   "source": [
    "# Optional: Alternatives to ReLU #\n",
    "\n",
    "There is a whole family of variants of the `'relu'` activation -- `'elu'`, `'selu'`, and `'swish'`, among others -- all of which you can use in Keras. Sometimes one activation will perform better than another on a given task, some may run faster, etc., so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems and is very fast, so it's a good one to start with.\n",
    "\n",
    "Let's look at the graphs of some of these. Change the activation from `'relu'` to one of the others named above. Then run the cell to see the graph. (Check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/activations) for more ideas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0779a8-3e07-42ae-acbe-4e7417f5821e",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\n",
    "activation_layer = layers.Activation('relu')\n",
    "\n",
    "x = tf.linspace(-3.0, 3.0, 100)\n",
    "y = activation_layer(x) # once created, a layer is callable just like a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305f7bc-91be-4f19-83c3-3047875d9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore for now\n",
    "\n",
    "# plt.figure(dpi=100)\n",
    "# plt.plot(x, y)\n",
    "# plt.xlim(-3, 3)\n",
    "# plt.xlabel(\"Input\")\n",
    "# plt.ylabel(\"Output\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.413477,
   "end_time": "2022-05-05T17:51:24.535852",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-05T17:51:06.122375",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
