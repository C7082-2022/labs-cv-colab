{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed2f0ba-8bca-4eb5-abe9-b3892b68be60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 05 Custom convnets\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"img/craiyon-custom.jpg\" alt=\"Spam\" width=\"300\"/> \n",
    "    \n",
    "    Craiyon.com \"Custom Job\" \n",
    "    \n",
    "</center>    \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade92be2-2683-4f06-803e-dd24044d0565",
   "metadata": {},
   "source": [
    "\n",
    "## 1 Objectives\n",
    "\n",
    "Now that you've seen the layers a convnet uses to extract features, it's time to put them together and build a network of your own!\n",
    "\n",
    "We have seen how convolutional networks perform **feature extraction** through three operations: **filter**, **detect**, and **condense**. A single round of feature extraction can only extract relatively simple features from an image, things like simple lines or contrasts. These are too simple to solve most classification problems. Instead, convnets will repeat this extraction over and over, so that the features become more complex and refined as they travel deeper into the network.\n",
    "\n",
    "<center><figure>\n",
    "<img src=\"img/5-1.png\" alt=\"Features extracted from an image of a car, from simple to refined.\" width=800>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4514f",
   "metadata": {
    "papermill": {
     "duration": 0.00327,
     "end_time": "2022-10-13T18:54:02.124414",
     "exception": false,
     "start_time": "2022-10-13T18:54:02.121144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## 2 Convolutional Blocks\n",
    "\n",
    "It does this by passing them through long chains of **convolutional blocks** which perform this extraction.\n",
    "\n",
    "<center><figure>\n",
    "<img src=\"img/5-2.png\" width=\"400\" alt=\"Extraction as a sequence of blocks.\">\n",
    "</figure></center>\n",
    "\n",
    "These convolutional blocks are stacks of `Conv2D` and `MaxPool2D` layers, whose role in feature extraction we learned about in the last few lessons.\n",
    "\n",
    "<center><figure>\n",
    "<!-- <img src=\"./images/2-block-crp.png\" width=\"400\" alt=\"A kind of extraction block: convolution, ReLU, pooling.\"> -->\n",
    "<img src=\"img/5-3.png\" width=\"400\" alt=\"A kind of extraction block: convolution, ReLU, pooling.\">\n",
    "</figure></center>\n",
    "\n",
    "Each block represents a round of extraction, and by composing these blocks the convnet can combine and recombine the features produced, growing them and shaping them to better fit the problem at hand. The deep structure of modern convnets is what allows this sophisticated feature engineering and has been largely responsible for their superior performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ea30c-5dd4-4b4b-a4f8-e824b85e80f8",
   "metadata": {},
   "source": [
    "## 3 Example - Design a Convnet\n",
    "\n",
    "Let's see how to define a deep convolutional network capable of engineering complex features. In this example, we'll create a Keras `Sequence` model and then train it on our Cars dataset.\n",
    "\n",
    "### 3.1 - Load Data ##\n",
    "\n",
    "This hidden cell loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc547891",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 16.007157,
     "end_time": "2022-10-13T18:54:18.133991",
     "exception": false,
     "start_time": "2022-10-13T18:54:02.126834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5117 files belonging to 2 classes.\n",
      "Found 5051 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Reproducability\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed()\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "\n",
    "# Load training and validation sets\n",
    "ds_train_ = image_dataset_from_directory(\n",
    "    'data/car-or-truck/train',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "ds_valid_ = image_dataset_from_directory(\n",
    "    'data/car-or-truck/valid',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = (\n",
    "    ds_train_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "ds_valid = (\n",
    "    ds_valid_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5801683",
   "metadata": {
    "papermill": {
     "duration": 0.002276,
     "end_time": "2022-10-13T18:54:18.139134",
     "exception": false,
     "start_time": "2022-10-13T18:54:18.136858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2 - Define Model\n",
    "\n",
    "Here is a diagram of the model we'll use:\n",
    "\n",
    "<center><figure>\n",
    "<!-- <img src=\"./images/2-convmodel-1.png\" width=\"200\" alt=\"Diagram of a convolutional model.\"> -->\n",
    "<img src=\"img/5-4.png\" width=\"250\" alt=\"Diagram of a convolutional model.\">\n",
    "</figure></center>\n",
    "\n",
    "Now we'll define the model. See how our model consists of three blocks of `Conv2D` and `MaxPool2D` layers (the base) followed by a head of `Dense` layers. We can translate this diagram more or less directly into a Keras `Sequential` model just by filling in the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7f6b39",
   "metadata": {
    "papermill": {
     "duration": 0.24438,
     "end_time": "2022-10-13T18:54:18.385952",
     "exception": false,
     "start_time": "2022-10-13T18:54:18.141572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 64, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 196614    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291,405\n",
      "Trainable params: 291,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same',\n",
    "                  # give the input dimensions in the first layer\n",
    "                  # [height, width, color channels(RGB)]\n",
    "                  input_shape=[128, 128, 3]),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Classifier Head\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=6, activation=\"relu\"),\n",
    "    layers.Dense(units=1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b4cb02",
   "metadata": {
    "papermill": {
     "duration": 0.002419,
     "end_time": "2022-10-13T18:54:18.391863",
     "exception": false,
     "start_time": "2022-10-13T18:54:18.389444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice in this definition is how the number of filters doubled block-by-block: 32, 64, 128. This is a common pattern. Since the `MaxPool2D` layer is reducing the *size* of the feature maps, we can afford to increase the *quantity* we create.\n",
    "\n",
    "### 3.3 - Train\n",
    "\n",
    "We can train this model: compile it with an optimizer along with a loss and metric appropriate for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7125a44a",
   "metadata": {
    "papermill": {
     "duration": 188.562356,
     "end_time": "2022-10-13T18:57:26.956785",
     "exception": false,
     "start_time": "2022-10-13T18:54:18.394429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "80/80 [==============================] - 34s 417ms/step - loss: 0.6832 - binary_accuracy: 0.5697 - val_loss: 0.6779 - val_binary_accuracy: 0.5785\n",
      "Epoch 2/40\n",
      "80/80 [==============================] - 34s 423ms/step - loss: 0.6690 - binary_accuracy: 0.5785 - val_loss: 0.6618 - val_binary_accuracy: 0.5785\n",
      "Epoch 3/40\n",
      "80/80 [==============================] - 32s 401ms/step - loss: 0.6608 - binary_accuracy: 0.5787 - val_loss: 0.6566 - val_binary_accuracy: 0.5785\n",
      "Epoch 4/40\n",
      "80/80 [==============================] - 32s 397ms/step - loss: 0.6565 - binary_accuracy: 0.5878 - val_loss: 0.6518 - val_binary_accuracy: 0.6120\n",
      "Epoch 5/40\n",
      "80/80 [==============================] - 31s 390ms/step - loss: 0.6523 - binary_accuracy: 0.6226 - val_loss: 0.6485 - val_binary_accuracy: 0.6145\n",
      "Epoch 6/40\n",
      "80/80 [==============================] - 32s 402ms/step - loss: 0.6426 - binary_accuracy: 0.6320 - val_loss: 0.6518 - val_binary_accuracy: 0.5989\n",
      "Epoch 7/40\n",
      "80/80 [==============================] - 32s 396ms/step - loss: 0.6352 - binary_accuracy: 0.6336 - val_loss: 0.6416 - val_binary_accuracy: 0.6155\n",
      "Epoch 8/40\n",
      "80/80 [==============================] - 32s 403ms/step - loss: 0.6243 - binary_accuracy: 0.6461 - val_loss: 0.6424 - val_binary_accuracy: 0.6100\n",
      "Epoch 9/40\n",
      "80/80 [==============================] - 32s 400ms/step - loss: 0.6154 - binary_accuracy: 0.6553 - val_loss: 0.6310 - val_binary_accuracy: 0.6339\n",
      "Epoch 10/40\n",
      "80/80 [==============================] - 30s 377ms/step - loss: 0.6053 - binary_accuracy: 0.6703 - val_loss: 0.6157 - val_binary_accuracy: 0.6609\n",
      "Epoch 11/40\n",
      "80/80 [==============================] - 29s 359ms/step - loss: 0.5925 - binary_accuracy: 0.6891 - val_loss: 0.5962 - val_binary_accuracy: 0.6735\n",
      "Epoch 12/40\n",
      "80/80 [==============================] - 30s 371ms/step - loss: 0.5763 - binary_accuracy: 0.7049 - val_loss: 0.5890 - val_binary_accuracy: 0.6824\n",
      "Epoch 13/40\n",
      "80/80 [==============================] - 30s 374ms/step - loss: 0.5593 - binary_accuracy: 0.7205 - val_loss: 0.5719 - val_binary_accuracy: 0.7010\n",
      "Epoch 14/40\n",
      "80/80 [==============================] - 30s 372ms/step - loss: 0.5360 - binary_accuracy: 0.7387 - val_loss: 0.5521 - val_binary_accuracy: 0.7203\n",
      "Epoch 15/40\n",
      "80/80 [==============================] - 30s 370ms/step - loss: 0.5099 - binary_accuracy: 0.7596 - val_loss: 0.5323 - val_binary_accuracy: 0.7357\n",
      "Epoch 16/40\n",
      "80/80 [==============================] - 30s 372ms/step - loss: 0.4852 - binary_accuracy: 0.7776 - val_loss: 0.5177 - val_binary_accuracy: 0.7444\n",
      "Epoch 17/40\n",
      "80/80 [==============================] - 30s 378ms/step - loss: 0.4592 - binary_accuracy: 0.7907 - val_loss: 0.4992 - val_binary_accuracy: 0.7579\n",
      "Epoch 18/40\n",
      "80/80 [==============================] - 33s 417ms/step - loss: 0.4325 - binary_accuracy: 0.8095 - val_loss: 0.4910 - val_binary_accuracy: 0.7660\n",
      "Epoch 19/40\n",
      "80/80 [==============================] - 33s 412ms/step - loss: 0.4050 - binary_accuracy: 0.8282 - val_loss: 0.5029 - val_binary_accuracy: 0.7579\n",
      "Epoch 20/40\n",
      "80/80 [==============================] - 33s 419ms/step - loss: 0.3806 - binary_accuracy: 0.8386 - val_loss: 0.5159 - val_binary_accuracy: 0.7517\n",
      "Epoch 21/40\n",
      "80/80 [==============================] - 33s 417ms/step - loss: 0.3498 - binary_accuracy: 0.8548 - val_loss: 0.5277 - val_binary_accuracy: 0.7555\n",
      "Epoch 22/40\n",
      "80/80 [==============================] - 34s 424ms/step - loss: 0.3209 - binary_accuracy: 0.8691 - val_loss: 0.5459 - val_binary_accuracy: 0.7553\n",
      "Epoch 23/40\n",
      "80/80 [==============================] - 33s 414ms/step - loss: 0.2944 - binary_accuracy: 0.8835 - val_loss: 0.5658 - val_binary_accuracy: 0.7632\n",
      "Epoch 24/40\n",
      "80/80 [==============================] - 34s 430ms/step - loss: 0.2595 - binary_accuracy: 0.9023 - val_loss: 0.5827 - val_binary_accuracy: 0.7694\n",
      "Epoch 25/40\n",
      "80/80 [==============================] - 34s 425ms/step - loss: 0.2345 - binary_accuracy: 0.9117 - val_loss: 0.6119 - val_binary_accuracy: 0.7676\n",
      "Epoch 26/40\n",
      "80/80 [==============================] - 34s 421ms/step - loss: 0.2095 - binary_accuracy: 0.9197 - val_loss: 0.6361 - val_binary_accuracy: 0.7739\n",
      "Epoch 27/40\n",
      "80/80 [==============================] - 34s 430ms/step - loss: 0.2012 - binary_accuracy: 0.9197 - val_loss: 0.5322 - val_binary_accuracy: 0.7985\n",
      "Epoch 28/40\n",
      "80/80 [==============================] - 34s 430ms/step - loss: 0.2365 - binary_accuracy: 0.9005 - val_loss: 0.8428 - val_binary_accuracy: 0.6913\n",
      "Epoch 29/40\n",
      "80/80 [==============================] - 34s 430ms/step - loss: 0.2080 - binary_accuracy: 0.9189 - val_loss: 0.7763 - val_binary_accuracy: 0.7094\n",
      "Epoch 30/40\n",
      "80/80 [==============================] - 34s 426ms/step - loss: 0.1999 - binary_accuracy: 0.9220 - val_loss: 0.5512 - val_binary_accuracy: 0.7731\n",
      "Epoch 31/40\n",
      "80/80 [==============================] - 33s 417ms/step - loss: 0.1887 - binary_accuracy: 0.9265 - val_loss: 0.6627 - val_binary_accuracy: 0.7721\n",
      "Epoch 32/40\n",
      "80/80 [==============================] - 34s 427ms/step - loss: 0.1479 - binary_accuracy: 0.9468 - val_loss: 0.6459 - val_binary_accuracy: 0.7899\n",
      "Epoch 33/40\n",
      "80/80 [==============================] - 33s 419ms/step - loss: 0.1053 - binary_accuracy: 0.9656 - val_loss: 0.6022 - val_binary_accuracy: 0.7992\n",
      "Epoch 34/40\n",
      "80/80 [==============================] - 32s 403ms/step - loss: 0.0894 - binary_accuracy: 0.9709 - val_loss: 0.6513 - val_binary_accuracy: 0.7880\n",
      "Epoch 35/40\n",
      "80/80 [==============================] - 32s 402ms/step - loss: 0.1130 - binary_accuracy: 0.9541 - val_loss: 0.7287 - val_binary_accuracy: 0.7850\n",
      "Epoch 36/40\n",
      "80/80 [==============================] - 33s 414ms/step - loss: 0.1000 - binary_accuracy: 0.9621 - val_loss: 0.9999 - val_binary_accuracy: 0.7505\n",
      "Epoch 37/40\n",
      "80/80 [==============================] - 34s 431ms/step - loss: 0.1005 - binary_accuracy: 0.9617 - val_loss: 0.8150 - val_binary_accuracy: 0.7909\n",
      "Epoch 38/40\n",
      "80/80 [==============================] - 33s 414ms/step - loss: 0.0722 - binary_accuracy: 0.9765 - val_loss: 0.8380 - val_binary_accuracy: 0.7832\n",
      "Epoch 39/40\n",
      "80/80 [==============================] - 33s 407ms/step - loss: 0.0766 - binary_accuracy: 0.9721 - val_loss: 0.8921 - val_binary_accuracy: 0.7846\n",
      "Epoch 40/40\n",
      "80/80 [==============================] - 32s 407ms/step - loss: 0.0915 - binary_accuracy: 0.9642 - val_loss: 0.7115 - val_binary_accuracy: 0.8070\n"
     ]
    }
   ],
   "source": [
    "# This one takes a while\n",
    "import tensorflow as tf\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=40,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcb075",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.593326,
     "end_time": "2022-10-13T18:57:27.553402",
     "exception": false,
     "start_time": "2022-10-13T18:57:26.960076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bf9df",
   "metadata": {
    "papermill": {
     "duration": 0.003374,
     "end_time": "2022-10-13T18:57:27.560461",
     "exception": false,
     "start_time": "2022-10-13T18:57:27.557087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This model is much smaller than the VGG16 model from Lesson 1 -- only 3 convolutional layers versus the 16 of VGG16. It was nevertheless able to fit this dataset fairly well. We might still be able to improve this simple model by adding more convolutional layers, hoping to create features better adapted to the dataset. This is what we'll try in the exercises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f924e-5c21-49d5-b140-52953acde976",
   "metadata": {},
   "source": [
    "## 4 Exercises\n",
    "\n",
    "In these exercises, you'll build a custom convnet with performance competitive to the VGG16 model from Lesson 1.\n",
    "\n",
    "Get started by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cac5df-d4ee-4ebf-8286-101219b4e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5117 files belonging to 2 classes.\n",
      "Found 5051 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Reproducability\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed()\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "\n",
    "# Load training and validation sets\n",
    "ds_train_ = image_dataset_from_directory(\n",
    "    'data/car-or-truck/train',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "ds_valid_ = image_dataset_from_directory(\n",
    "    'data/car-or-truck/valid',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = (\n",
    "    ds_train_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "ds_valid = (\n",
    "    ds_valid_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2649dd-c31c-41be-8c56-2379e86587a0",
   "metadata": {},
   "source": [
    "**Design a Convnet**\n",
    "\n",
    "Let's design a convolutional network with a block architecture like we saw in the tutorial. The model from the example had three blocks, each with a single convolutional layer. Its performance on the \"Car or Truck\" problem was okay, but far from what the pretrained VGG16 could achieve. It might be that our simple network lacks the ability to extract sufficiently complex features. We could try improving the model either by adding more blocks or by adding convolutions to the blocks we have.\n",
    "\n",
    "Let's go with the second approach. We'll keep the three block structure, but increase the number of `Conv2D` layer in the second block to two, and in the third block to three.\n",
    "\n",
    "<center><figure>\n",
    "<!-- <img src=\"./images/2-convmodel-2.png\" width=\"250\" alt=\"Diagram of a convolutional model.\"> -->\n",
    "<img src=\"img/5-5.png\" width=\"250\" alt=\"Diagram of a convolutional model.\">\n",
    "</figure></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c2056-98f4-4f0b-b1df-03b6d0c1d8c2",
   "metadata": {},
   "source": [
    "### 4.1 Define Model \n",
    "\n",
    "Given the diagram above, complete the model by defining the layers of the third block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83f181b-3680-4ef2-9a54-d55ca1f71c27",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Block One\n",
    "    layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same',\n",
    "                  input_shape=[128, 128, 3]),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    # YOUR CODE HERE\n",
    "    # ____,\n",
    "\n",
    "    # Head\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0fadc-499a-4f15-9fa4-ba03c81535e8",
   "metadata": {},
   "source": [
    "### 4.2 Compile\n",
    "\n",
    "To prepare for training, compile the model with an appropriate loss and accuracy metric for the \"Car or Truck\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd6251b-a738-4501-a444-f57d86dfd411",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n",
    "    # YOUR CODE HERE: Add loss and metric\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab52fe5-0758-4be2-a637-75a25577e387",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb053d93-d47b-4042-8e81-6410c0ab9a25",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Finally, let's test the performance of this new model. First run this cell to fit the model to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b6cd2-7c74-4fe5-b6e8-48234b1495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca03f7-6605-4239-aae9-e98ce009e58a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And now run the cell below to plot the loss and metric curves for this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197a935-2f7a-4040-82d3-7f97606ba8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f858113-5c27-4dbb-ac03-6af516d459df",
   "metadata": {},
   "source": [
    "### 4.3 Train the Model #\n",
    "\n",
    "How would you interpret these training curves? Did this model improve upon the model from the tutorial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd1980-1993-4fdb-b5c3-a370bd50be3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 216.370079,
   "end_time": "2022-10-13T18:57:30.873527",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-13T18:53:54.503448",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
